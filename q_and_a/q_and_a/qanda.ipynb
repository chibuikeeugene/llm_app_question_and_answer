{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our llm model - Note the ollama server should be running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our llm model\n",
    "llm = Ollama(model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A great question!\\n\\nLinguamatic (not Langsmith, but I assume it's a similar concept) is an AI-powered language generation platform that can indeed be leveraged to facilitate testing in various ways. Here are some examples:\\n\\n1. **Automated Testing**: Linguamatic can generate test cases, input data, or expected output based on natural language descriptions. This can save time and effort in creating test scenarios.\\n2. **Test Data Generation**: The platform can produce a wide range of realistic test data (e.g., text, numbers, dates) to simulate various user inputs, allowing you to cover more testing scenarios.\\n3. **Error Detection**: Linguamatic's natural language processing capabilities can identify potential errors or inconsistencies in your code, helping you catch bugs early on.\\n4. **Testing for Language-Specific Features**: If your application has features that rely heavily on language understanding (e.g., text analysis, sentiment analysis), Linguamatic can assist in testing these features by generating realistic input data and expected output.\\n5. **Test Case Generation**: The platform's AI engine can analyze your test cases and generate new ones based on the same logic, ensuring thorough coverage of all possible scenarios.\\n6. **Automated Testing for NLP-based Features**: Linguamatic can be used to test features that rely on Natural Language Processing (NLP) techniques, such as text classification, entity recognition, or language translation.\\n\\nTo get started with using Linguamatic for testing, you'll need to:\\n\\n1. Integrate the platform's API into your existing testing framework.\\n2. Provide the necessary input data and expected output for the test cases you want to automate.\\n3. Configure the platform's settings to customize the testing process (e.g., language support, testing scope).\\n\\nKeep in mind that Linguamatic is a powerful tool, and proper training and setup are essential to ensure accurate results. Additionally, some testing scenarios might require human judgment or review, so it's crucial to balance automation with manual testing efforts.\\n\\nI hope this helps!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('how can langsmith help with testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Using a Chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laoding a template to help refine our model response\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"you are a renowned technical writer\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chaining our prompt with the llm\n",
    "chain = prompt | llm\n",
    "\n",
    "# invoking the llm\n",
    "response = chain.invoke({\"input\": \"how can langsmith help with testing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a technical writer, I'm delighted to share some insights on how Langsmith can assist with testing.\n",
      "\n",
      "Langsmith is an excellent tool for testing machine learning models. Here are some ways it can help:\n",
      "\n",
      "1. **Automated Testing**: Langsmith provides automated testing capabilities, which means you can run tests repeatedly without manual intervention. This helps ensure that your model's performance remains consistent across different inputs and scenarios.\n",
      "2. **Model Interpretability**: Langsmith allows you to visualize the internal workings of your machine learning models. By analyzing feature importance, partial dependence plots, and SHAP values, you can gain insights into how the model is making predictions. This helps identify potential biases or errors in the model's behavior.\n",
      "3. **Edge Case Detection**: Langsmith's ability to generate synthetic data allows you to test edge cases that might not be easily replicable through manual testing. By feeding these generated inputs into your model, you can ensure it handles unusual or extreme scenarios correctly.\n",
      "4. **Model Performance Analysis**: Langsmith provides metrics and visualizations to help evaluate the performance of your machine learning models. This includes metrics like accuracy, precision, recall, F1-score, AUC-ROC, and more. You can use these insights to identify areas for improvement in your model's training or tuning.\n",
      "5. **Collaboration**: Langsmith offers a collaborative testing environment that allows multiple stakeholders to review and validate test results. This ensures that everyone is on the same page regarding the performance of your machine learning models.\n",
      "\n",
      "To get started with Langsmith, you can follow these general steps:\n",
      "\n",
      "1. Install Langsmith and familiarize yourself with its interface.\n",
      "2. Prepare your machine learning model for testing by implementing a suitable API or input/output format.\n",
      "3. Define test cases using Langsmith's syntax or by generating synthetic data.\n",
      "4. Run tests and analyze the results to identify areas for improvement in your model's training or tuning.\n",
      "\n",
      "By leveraging Langsmith, you can streamline your testing process, reduce manual effort, and gain more confidence in the performance of your machine learning models.\n"
     ]
    }
   ],
   "source": [
    "print(response\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing a output parser to refine the response of our llm model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a technical writer, I'm excited to share the ways Langsmith can facilitate effective testing for your project.\\n\\n**Automated Testing**: Langsmith's ability to generate natural language tests (NLTs) can significantly reduce the effort required to create automated test cases. By leveraging Langsmith's AI-powered testing capabilities, you can:\\n\\n1. **Quickly generate test data**: Create realistic test data that accurately reflects real-world scenarios.\\n2. **Automate tedious testing tasks**: Langsmith can automatically execute repetitive tests, freeing up your team to focus on more complex and creative testing efforts.\\n\\n**Test Case Generation**: Langsmith's AI-powered language processing capabilities allow it to analyze requirements documents, user stories, or acceptance criteria to generate test cases. This can help:\\n\\n1. **Ensure comprehensive coverage**: Generate a set of test cases that thoroughly cover the required scenarios.\\n2. **Reduce manual testing effort**: Automate the process of creating test cases, reducing the time and resources needed.\\n\\n**Test Data Generation**: Langsmith's NLT capabilities enable the generation of realistic data for testing, which can help:\\n\\n1. **Simulate real-world scenarios**: Create test data that accurately reflects actual user behavior or environmental conditions.\\n2. **Reduce testing complexity**: Simplify testing by providing relevant and meaningful test data, rather than relying on manual data creation.\\n\\n**Integration with CI/CD Pipelines**: Langsmith's API-based integration allows for seamless integration with Continuous Integration/Continuous Deployment (CI/CD) pipelines. This enables:\\n\\n1. **Automated testing in the pipeline**: Run automated tests as part of your CI/CD workflow, ensuring that changes are thoroughly tested before deployment.\\n2. **Faster feedback loops**: Receive immediate feedback on test results, reducing the time and effort required to identify and fix issues.\\n\\nBy leveraging Langsmith's capabilities, you can streamline your testing process, reduce manual testing efforts, and ensure that your software meets the highest quality standards.\\n\\nHow would you like to utilize Langsmith for testing?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding a output parser to convert our chat message to a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# chain the output parser to the chat model\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "#call the chain invoke method to get a response\n",
    "response = chain.invoke({\"input\": \"how can langsmith help with testing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"As a technical writer, I'm excited to share the ways Langsmith can facilitate effective testing for your project.\",\n",
       " \"**Automated Testing**: Langsmith's ability to generate natural language tests (NLTs) can significantly reduce the effort required to create automated test cases. By leveraging Langsmith's AI-powered testing capabilities, you can:\",\n",
       " '1. **Quickly generate test data**: Create realistic test data that accurately reflects real-world scenarios.\\n2. **Automate tedious testing tasks**: Langsmith can automatically execute repetitive tests, freeing up your team to focus on more complex and creative testing efforts.',\n",
       " \"**Test Case Generation**: Langsmith's AI-powered language processing capabilities allow it to analyze requirements documents, user stories, or acceptance criteria to generate test cases. This can help:\",\n",
       " '1. **Ensure comprehensive coverage**: Generate a set of test cases that thoroughly cover the required scenarios.\\n2. **Reduce manual testing effort**: Automate the process of creating test cases, reducing the time and resources needed.',\n",
       " \"**Test Data Generation**: Langsmith's NLT capabilities enable the generation of realistic data for testing, which can help:\",\n",
       " '1. **Simulate real-world scenarios**: Create test data that accurately reflects actual user behavior or environmental conditions.\\n2. **Reduce testing complexity**: Simplify testing by providing relevant and meaningful test data, rather than relying on manual data creation.',\n",
       " \"**Integration with CI/CD Pipelines**: Langsmith's API-based integration allows for seamless integration with Continuous Integration/Continuous Deployment (CI/CD) pipelines. This enables:\",\n",
       " '1. **Automated testing in the pipeline**: Run automated tests as part of your CI/CD workflow, ensuring that changes are thoroughly tested before deployment.\\n2. **Faster feedback loops**: Receive immediate feedback on test results, reducing the time and effort required to identify and fix issues.',\n",
       " \"By leveraging Langsmith's capabilities, you can streamline your testing process, reduce manual testing efforts, and ensure that your software meets the highest quality standards.\",\n",
       " 'How would you like to utilize Langsmith for testing?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.rsplit(sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Arguably it seems without the outputparser the initial response outlook was far more better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Retriever (a vectorstore) that would serve as our body of Knoweledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Setup your user-agent environment variable to use the webbaseloader \n",
    "# Try out the command - curl -s -I https://www.google.com | grep \"User-Agent\"\n",
    "# This will return the user-agent used by curl to access the website. Note the default browser in question is what is used.\n",
    "# Another alternative could be to use chrome developer tool to obtain your user-agent environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing our docs into a vectorstore\n",
    "# first create our embedding object\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings  = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can use this embedding model to ingest documents into a vectorstore. \n",
    "# We will use a simple local vectorstore, FAISS, for now\n",
    "# For FAISS, install the library faiss-cpu\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Building our index\n",
    "\n",
    "# instantiate our text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have this data indexed in a vectorstore, we will create a retrieval chain. This chain will take an incoming question, \n",
    "# look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.\n",
    "\n",
    "\n",
    "# Setting up a chain that takes a question and the retrived document and generates an answer\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "                                 \n",
    "Question: {input}\"\"\")\n",
    "\n",
    "\n",
    "document_chain = create_stuff_documents_chain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our retrieval chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "\n",
    "# we call the invoke method on the retrieval chain, passing in a query\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, LangSmith helps with testing in the following ways:\n",
      "\n",
      "1. **Test cases**: LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\n",
      "2. **Comparison view**: LangSmith provides a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.\n",
      "3. **Automated evaluations**: Langsmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.\n",
      "\n",
      "Overall, LangSmith helps developers create, manage, and evaluate test cases to ensure their LLM applications are performing correctly and consistently.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding our retrieval chain into a conversational retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing functions to help create and handle chat history\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "\n",
    "# updating our chat prompt template\n",
    "prompt = ChatPromptTemplate([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "retriever_chain_with_history = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  testing out our retriever chain with history\n",
    "# creating an instance of a follow up question\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}, page_content='meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of and annotate your application across multiple turns.Was this page helpful?You can leave detailed feedback on GitHub.PreviousQuick StartNextOverviewPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}, page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookThis is outdated documentation for 🦜️🛠️ LangSmith, which is no longer actively maintained.For up-to-date documentation, see the latest version.User GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.Prototyping\\u200bPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing — and debug where it is failing — is incredibly important for this phase.Debugging\\u200bWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn’t necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it’s extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\\u200bWhile many developers still ship an initial version of their application based on “vibe checks”, we’ve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View\\u200bWhen prototyping different versions of your applications and making changes, it’s important to see whether or not you’ve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it’s useful to be able to view results for different configurations on the same datapoints side-by-side. We’ve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground\\u200bLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}, page_content='LangSmith User Guide | 🦜️🛠️ LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}, page_content=\"Every playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing\\u200bBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it’s important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it’s breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback\\u200bWhen launching your application to an initial set of users, it’s important to gather human feedback on the responses it’s producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces\\u200bLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset\\u200bAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production\\u200bClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you’ll also want to do once your app hits production.However, especially at the production stage, it’s crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Online evaluations and automations allow you to process and score production traces in near real-time.Additionally, threads provide a seamless way to group traces from a single conversation, making it easier to track the performance of your application across multiple turns.Monitoring and A/B Testing\\u200bLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period — this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Automations\\u200bAutomations are a powerful feature in LangSmith that allow you to perform actions on traces in near real-time. This can be used to automatically score traces, send them to annotation queues, or send them to datasets.To define an automation, simply provide a filter condition, a sampling rate, and an action to perform. Automations are particularly helpful for processing traces at production scale.Threads\\u200bMany LLM applications are multi-turn, meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to\")]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content= \"Can LangSmith help test my LLM applications?\"), AIMessage(content = \"Yes !\")]\n",
    "retriever_chain_with_history.invoke(\n",
    "    {\n",
    "        \"chat_history\": chat_history,\n",
    "        \"input\": \"Tell me how?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have this new retriever, we can create a new chain to continue \n",
    "# the conversation with these retrieved documents in mind.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain_update = create_retrieval_chain(retriever_chain_with_history, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes !')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}, page_content='meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of and annotate your application across multiple turns.Was this page helpful?You can leave detailed feedback on GitHub.PreviousQuick StartNextOverviewPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}, page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookThis is outdated documentation for 🦜️🛠️ LangSmith, which is no longer actively maintained.For up-to-date documentation, see the latest version.User GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.Prototyping\\u200bPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing — and debug where it is failing — is incredibly important for this phase.Debugging\\u200bWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn’t necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it’s extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\\u200bWhile many developers still ship an initial version of their application based on “vibe checks”, we’ve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View\\u200bWhen prototyping different versions of your applications and making changes, it’s important to see whether or not you’ve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it’s useful to be able to view results for different configurations on the same datapoints side-by-side. We’ve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground\\u200bLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.'),\n",
       "  Document(metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}, page_content='LangSmith User Guide | 🦜️🛠️ LangSmith'),\n",
       "  Document(metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}, page_content=\"Every playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing\\u200bBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it’s important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it’s breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback\\u200bWhen launching your application to an initial set of users, it’s important to gather human feedback on the responses it’s producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces\\u200bLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset\\u200bAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production\\u200bClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you’ll also want to do once your app hits production.However, especially at the production stage, it’s crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Online evaluations and automations allow you to process and score production traces in near real-time.Additionally, threads provide a seamless way to group traces from a single conversation, making it easier to track the performance of your application across multiple turns.Monitoring and A/B Testing\\u200bLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period — this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Automations\\u200bAutomations are a powerful feature in LangSmith that allow you to perform actions on traces in near real-time. This can be used to automatically score traces, send them to annotation queues, or send them to datasets.To define an automation, simply provide a filter condition, a sampling rate, and an action to perform. Automations are particularly helpful for processing traces at production scale.Threads\\u200bMany LLM applications are multi-turn, meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to\")],\n",
       " 'answer': \"LangSmith is designed to support various stages of your application development lifecycle, including prototyping, beta testing, and production. Here's how you can leverage LangSmith to test your LLM applications:\\n\\n1. **Prototyping**: Use the playground environment for rapid iteration and experimentation with different prompts and models.\\n2. **Debugging**: Enable tracing by default, which will provide clear visibility and debugging information at each step of an LLM sequence. This helps identify and root-cause issues when things go wrong.\\n3. **Initial Test Set**: Create datasets, which are collections of inputs and reference outputs, to run tests on your LLM applications. You can upload test cases in bulk or create them on the fly, and export them from application traces.\\n4. **Comparison View**: Use this view to track and diagnose regressions in test scores across multiple revisions of your application.\\n5. **Beta Testing**: Collect feedback on how your app is performing in real-world scenarios by gathering human feedback on responses it's producing. This helps draw attention to edge cases that are causing problematic responses.\\n6. **Capturing Feedback**: Attach feedback scores to logged traces and filter on traces with a specific feedback tag and score.\\n7. **Annotating Traces**: Send runs to annotation queues, where annotators can inspect interesting traces and annotate them with respect to different criteria.\\n\\nAdditionally, LangSmith offers various features to support testing at the production stage:\\n\\n1. **Monitoring and A/B Testing**: Use monitoring charts to track key metrics over time, drill down into specific data points, and group versions of your application for side-by-side comparison.\\n2. **Automations**: Automate actions on traces in near real-time, such as scoring, sending to annotation queues, or adding to datasets.\\n\\nOverall, LangSmith provides a comprehensive platform for LLM application development, monitoring, and testing, making it easier to track the performance of your application across multiple turns.\"}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing end-2-end\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes !\")]\n",
    "retrieval_chain_update.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, LangSmith can help test your LLM (Large Language Model) applications in several ways:\n",
      "\n",
      "1. **Prototyping**: LangSmith allows rapid experimentation between prompts, model types, retrieval strategy, and other parameters. You can quickly test out different prompts and models in the playground environment.\n",
      "\n",
      "2. **Debugging**: When things go wrong, Langsmith gives clear visibility and debugging information at each step of an LLM sequence, making it easier to identify and root-cause issues.\n",
      "\n",
      "3. **Initial Test Set**: You can create datasets, which are collections of inputs and reference outputs, and use these to run tests on your LLM applications. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic-based) to score test results.\n",
      "\n",
      "4. **Comparison View**: When prototyping different versions of your applications and making changes, Langsmith provides a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.\n",
      "\n",
      "5. **Beta Testing**: Langsmith allows you to collect more data on how your LLM applications are performing in real-world scenarios. You can gather human feedback on the responses it's producing, filter on traces that have a specific feedback tag and score, and drill down into problematic points using the detailed trace view.\n",
      "\n",
      "6. **Production**: Once your app hits production, Langsmith provides monitoring charts to track key metrics over time, allowing you to get a high-level overview of application performance with respect to latency, cost, and feedback scores.\n",
      "\n",
      "7. **Automations**: Automations are a powerful feature in Langsmith that allow you to perform actions on traces in near real-time. This can be used to automatically score traces, send them to annotation queues, or send them to datasets.\n",
      "\n",
      "These are just some of the ways Langsmith can help test your LLM applications.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain_update.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent needs two important tools:\n",
    " - a retriever \n",
    " - a search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a retriever tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"retriever\",\n",
    "    description=\"Searches and returns information about the most relevant document based on the user's query.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up our search tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# to load our api-key for tavily from our environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the complete tooling\n",
    "tool = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing some key packages for the project\n",
    "# the following can be installed - langchainhub, langchain-ollama\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eugene/Personal_Projects/ML_Project/llm_app_question_and_answer/.venv/lib/python3.10/site-packages/langchain/hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3\",  \n",
    "    temperature = 0.8,  \n",
    "    num_predict = 256, \n",
    ")\n",
    "agent = create_openai_functions_agent(\n",
    "    llm, \n",
    "    tool, \n",
    "    prompt\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tool, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLinguine is an AI-powered language understanding platform that can significantly aid in testing various aspects of your product or service. Here are some ways Linguine can assist:\n",
      "\n",
      "1. **Natural Language Processing (NLP) Testing**: Linguine can be used to test the NLP capabilities of your application, such as text classification, sentiment analysis, entity recognition, and more. You can use Linguine's API to feed your application a wide range of texts with different characteristics, styles, and nuances.\n",
      "\n",
      "2. **Content Generation**: Linguine can generate large amounts of text data in various formats, languages, and styles. This can be useful for testing content-based applications, such as AI-powered chatbots or recommendation systems.\n",
      "\n",
      "3. **Edge Case Testing**: Linguine's ability to understand language patterns and variations enables you to test edge cases that might be difficult to identify manually. For instance, you can use Linguine to generate texts with specific characteristics, like sarcasm, irony, or idioms, to test your application's ability to handle these nuances.\n",
      "\n",
      "4. **Localization Testing**: Linguine supports multiple languages and dialects, making it an excellent tool for testing the localization aspects of your product or service. You can use Linguine to generate text data in various languages, cultures,\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': \"Linguine is an AI-powered language understanding platform that can significantly aid in testing various aspects of your product or service. Here are some ways Linguine can assist:\\n\\n1. **Natural Language Processing (NLP) Testing**: Linguine can be used to test the NLP capabilities of your application, such as text classification, sentiment analysis, entity recognition, and more. You can use Linguine's API to feed your application a wide range of texts with different characteristics, styles, and nuances.\\n\\n2. **Content Generation**: Linguine can generate large amounts of text data in various formats, languages, and styles. This can be useful for testing content-based applications, such as AI-powered chatbots or recommendation systems.\\n\\n3. **Edge Case Testing**: Linguine's ability to understand language patterns and variations enables you to test edge cases that might be difficult to identify manually. For instance, you can use Linguine to generate texts with specific characteristics, like sarcasm, irony, or idioms, to test your application's ability to handle these nuances.\\n\\n4. **Localization Testing**: Linguine supports multiple languages and dialects, making it an excellent tool for testing the localization aspects of your product or service. You can use Linguine to generate text data in various languages, cultures,\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
